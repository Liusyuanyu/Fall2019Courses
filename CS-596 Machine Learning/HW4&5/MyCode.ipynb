{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"%run -i \\\"D:\\\\Courses\\\\2019 Fall (Processing)\\\\CS-596 Machine Learning\\\\Assignments\\\\Assignment 3\\\\My Code\\\\main_part1.py\\\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \\\"D:\\\\Courses\\\\2019 Fall (Processing)\\\\CS-596 Machine Learning\\\\Assignments\\\\Assignment 3\\\\My Code\\\\main_part1.py\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from PartI_FNN.util import func_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (downloaded if needed) the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# transform each image from 28 by28 to a 784 pixel vector\n",
    "pixel_count = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0], pixel_count).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], pixel_count).astype('float32')\n",
    "\n",
    "# normalize inputs from gray scale of 0-255 to values between 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Please write your own codes in responses to the homework assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_train)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split( \\\n",
    "    x_train, y_train, test_size=10000, random_state=34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement FNN Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import deque\n",
    "\n",
    "class FNN_Model:\n",
    "    \"\"\"\n",
    "    neural network model\n",
    "    \"\"\"\n",
    "    def __init__(self, classes_num, IMAGE_PIXELS, params={}):\n",
    "        self.classes_num = classes_num\n",
    "        self.image_pixels = IMAGE_PIXELS\n",
    "        self.LEARNING_RATE = params['LearningRate']\n",
    "        self.reg = params['reg']\n",
    "\n",
    "        self.num_hidden = params['HiddenLayerNum']\n",
    "        self.hidden_size = params['HiddenLayerSize']\n",
    "        self.log_file_path = params['log_file_path']\n",
    "        tf.keras.backend.clear_session()\n",
    "        self.session = self.create_model()\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        images_placeholder = tf.placeholder(tf.float32, shape = (None, self.image_pixels))\n",
    "        labels_placeholder = tf.placeholder(tf.int64, shape = (None, self.classes_num))\n",
    "        return images_placeholder, labels_placeholder\n",
    "\n",
    "    def variable_summaries(self,var, name):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.summary.scalar('sttdev/' + name, stddev)\n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))\n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))\n",
    "\n",
    "    def multilayer_perceptron(self, input_mat):\n",
    "        n_input = self.image_pixels\n",
    "        n_hidden_1 = self.hidden_size\n",
    "        n_hidden_2 = self.hidden_size\n",
    "        n_classes = self.classes_num\n",
    "\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev = 0.1)),\n",
    "            'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev = 0.1)),\n",
    "            'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],stddev = 0.1))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([n_hidden_1], stddev = 0.1)),\n",
    "            'b2': tf.Variable(tf.random_normal([n_hidden_2], stddev = 0.1)),\n",
    "            'out': tf.Variable(tf.random_normal([n_classes], stddev = 0.1))\n",
    "        }\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(input_mat, weights['h1']), biases['b1'])\n",
    "        # mean,variance = tf.nn.moments(layer_1,[0])\n",
    "        # layer_1 = tf.nn.batch_normalization(layer_1,mean,variance,None,None,0.00001)\n",
    "        # layer_1 = tf.tanh(layer_1)\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        # mean,variance = tf.nn.moments(layer_2,[0])\n",
    "        # layer_2 = tf.nn.batch_normalization(layer_2,mean,variance,None,None,0.00001)\n",
    "        # layer_2 = tf.tanh(layer_2)\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        reg_term = self.reg * (tf.reduce_sum(tf.square(weights['h1']))+\n",
    "        tf.reduce_sum(tf.square(weights['h2']))+tf.reduce_sum(tf.square(weights['out'])))\n",
    "        return out_layer, reg_term, weights, biases\n",
    "\n",
    "    def create_model(self):\n",
    "        self.images_placeholder, self.labels_placeholder = self.define_placeholders()\n",
    "        output, reg_term, weights, biases = self.multilayer_perceptron(self.images_placeholder)\n",
    "\n",
    "        self.variable_summaries(weights['h1'], 'layer1'+ '/weights')\n",
    "        self.variable_summaries(weights['h2'], 'layer2' + '/weights')\n",
    "        self.variable_summaries(weights['out'], 'layer3' + '/weights')\n",
    "\n",
    "        self.NN_output = output\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=self.labels_placeholder))\n",
    "\n",
    "        tf.summary.scalar('cost', self.cost)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.LEARNING_RATE)\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        \"\"\"\n",
    "        Default runs at cpu,\n",
    "\n",
    "        For GPU runnning, it takes 33 percent of entire gpu memory\n",
    "        use this:\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "        session = tf.Session(config=tf.ConfigProto(gpu_options = gpu_options))\n",
    "        \"\"\"\n",
    "        session = tf.Session()\n",
    "        session.run(init)\n",
    "        self.saver_op = tf.train.Saver()\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter('dqn_log_files/log1/data3',session.graph)\n",
    "        return session\n",
    "\n",
    "    def init_t(self):\n",
    "        self.t = 0\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # print(path)\n",
    "        save_path = self.saver_op.save(self.session, path)\n",
    "        print ('Model saved in file:%s' %save_path)\n",
    "        # pass\n",
    "\n",
    "    def restore_model(self, path):\n",
    "        self.saver_op.restore(self.session, path)\n",
    "\n",
    "\n",
    "    def train_step(self, Input_mat, labels):\n",
    "        _, cost, prediction_probs, self.summary = self.session.run(\n",
    "        [self.train_op, self.cost, self.NN_output, self.merged],\n",
    "        feed_dict = {\n",
    "            self.images_placeholder : Input_mat,\n",
    "            self.labels_placeholder : labels\n",
    "        })\n",
    "        return cost\n",
    "\n",
    "    def predict(self, states, label):\n",
    "        cost, prediction_probs = self.session.run(\n",
    "        [self.cost, self.NN_output],\n",
    "        feed_dict = {\n",
    "        self.images_placeholder: states,\n",
    "        self.labels_placeholder: label\n",
    "        })\n",
    "        return prediction_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE =  0.001\n",
    "MAX_STEP = 2000\n",
    "HIDDEN_LAYER_SIZE = 120\n",
    "HIDDEN_LAYER_NUM = 2\n",
    "BATCH_SIZE = 400\n",
    "TRAIN_FIR = 'tf_logs'\n",
    "REGULARIZATION = 0.1\n",
    "\n",
    "# DEFAULT_SAVER_PATH = 'model/'\n",
    "# DEFAULT_LOG_PATH = \"dqn_log/log1/data1\"\n",
    "# DEFAULT_SAVING_FREQUENCY = 100\n",
    "\n",
    "# # import os\n",
    "# # import argparse\n",
    "\n",
    "# DEFAULT_EPI = 10000000\n",
    "# DEFAULT_STEPS = 500\n",
    "# DEFAULT_MEM_WIDTH = 100000\n",
    "# DEFAULT_INITIAL_EPSILON = 1\n",
    "# DEFAULT_FINAL_EPSILON = 0.1\n",
    "# DEFAULT_GAMMA = 0.95\n",
    "# DEFAULT_MINI_BATCH = 16\n",
    "# DEFAULT_OBSERVATION = 10000\n",
    "\n",
    "# DEFAULT_LEARNING_RATE = 0.000001\n",
    "# DEFAULT_REGULARIZATION = 0.001\n",
    "# DEFAULT_HIDDEN_LAYER = 300\n",
    "# DEFAULT_HIDDEN_LAYER_NUM = 2\n",
    "\n",
    "# DEFAULT_SAVER_PATH = 'model/'\n",
    "# DEFAULT_LOG_PATH = \"dqn_log/log1/data1\"\n",
    "# DEFAULT_SAVING_FREQUENCY = 100\n",
    "\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-episodes', default = DEFAULT_EPI, type = int)\n",
    "#     parser.add_argument('-steps', default = DEFAULT_STEPS, type = int )\n",
    "#     parser.add_argument('-MemSize', default = DEFAULT_MEM_WIDTH, type = int )\n",
    "#     parser.add_argument('-initial_epsilon', default = DEFAULT_INITIAL_EPSILON, type = int )\n",
    "#     parser.add_argument('-final_epsilon', default = DEFAULT_FINAL_EPSILON, type = int )\n",
    "#     parser.add_argument('-gamma', default = DEFAULT_GAMMA, type = int )\n",
    "#     parser.add_argument('-BatchSize', default = DEFAULT_MINI_BATCH, type = int )\n",
    "#     parser.add_argument('-reg', default = DEFAULT_REGULARIZATION, type = int )\n",
    "#     parser.add_argument('-LearningRate', default = DEFAULT_LEARNING_RATE, type = int )\n",
    "#     parser.add_argument('-HiddenLayerNum', default = DEFAULT_HIDDEN_LAYER_NUM, type = int )\n",
    "#     parser.add_argument('-HiddenLayerSize', default = DEFAULT_HIDDEN_LAYER, type = int )\n",
    "#     parser.add_argument('-observation', default = DEFAULT_OBSERVATION, type = int )\n",
    "#     parser.add_argument('-saver_path', default = DEFAULT_SAVER_PATH, type = str )\n",
    "#     parser.add_argument('-saving_rate', default = DEFAULT_SAVING_FREQUENCY, type = int)\n",
    "#     parser.add_argument('-log_path', default = DEFAULT_LOG_PATH, type = str)\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     training_params = {\n",
    "#         'saver_path' : args.saver_path,\n",
    "#         'saving_rate' : args.saving_rate\n",
    "#     }\n",
    "#     network_params = {\n",
    "#         'HiddenLayerSize': args.HiddenLayerSize,\n",
    "#         'reg':args.reg,\n",
    "#         'LearningRate': args.LearningRate,\n",
    "#         'HiddenLayerNum': args.HiddenLayerNum,\n",
    "#         'mini_batch_size': args.BatchSize,\n",
    "#         'log_file_path': args.log_path\n",
    "#         }\n",
    "\n",
    "#     return training_params, network_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_LAYER_SIZE = 2\n",
    "# INPUT_LAYER_SIZE = 73\n",
    "# BatchSize = 8\n",
    "\n",
    "training_epoch = 10\n",
    "LEARNING_RATE =  0.001\n",
    "MAX_STEP = 2000\n",
    "HIDDEN_LAYER_SIZE = 120\n",
    "HIDDEN_LAYER_NUM = 2\n",
    "# BATCH_SIZE = 400\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_DIR = 'tf_logs'\n",
    "REGULARIZATION = 0.1\n",
    "\n",
    "def import_data_training():\n",
    "    # load (downloaded if needed) the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # transform each image from 28 by28 to a 784 pixel vector\n",
    "    pixel_count = x_train.shape[1] * x_train.shape[2]\n",
    "    x_train = x_train.reshape(x_train.shape[0], pixel_count).astype('float32')\n",
    "    # normalize inputs from gray scale of 0-255 to values between 0-1\n",
    "    x_train = x_train / 255\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=10000, random_state=34)\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    int_encoded = le.fit_transform(y_train)\n",
    "    oe = preprocessing.OneHotEncoder(categories='auto')\n",
    "    oe.fit(int_encoded.reshape(len(int_encoded), 1))\n",
    "    y_train = oe.transform(y_train.reshape(len(y_train),1)).toarray()\n",
    "    y_test = oe.transform(y_test.reshape(len(y_test),1)).toarray()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "def save_nn_model(FNN, SAVING_PATH):\n",
    "    FNN.save_model(SAVING_PATH+ 'model.ckpt')\n",
    "\n",
    "def restore_nn_model(FNN, SAVING_PATH):\n",
    "    ckpt = tf.train.get_checkpoint_state(SAVING_PATH)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        FNN.restore_model(ckpt.model_checkpoint_path)\n",
    "        FNN.init_t()\n",
    "        print('saved model found and restored')\n",
    "    else:\n",
    "        print('No saved model, creating new one...')\n",
    "\n",
    "def write_to_log(FNN, t):\n",
    "    FNN.train_writer.add_summary(FNN.summary,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, cost = 2.2348036742210398\n",
      "Epoch: 1, cost = 2.0383416166305532\n",
      "Epoch: 2, cost = 1.8308837878704072\n",
      "Epoch: 3, cost = 1.6014531509876229\n",
      "Epoch: 4, cost = 1.3732412877082814\n",
      "Epoch: 5, cost = 1.1730413024425514\n",
      "Epoch: 6, cost = 1.0128371433019634\n",
      "Epoch: 7, cost = 0.8903929674625402\n"
     ]
    }
   ],
   "source": [
    "\"\"\"load the parameters\"\"\"\n",
    "# SAVING_PATH = training_params['saver_path']\n",
    "# SAVING_RATE = training_params['saving_rate']\n",
    "# input_data, output_data = import_data_training()\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "x_train, y_train, x_test, y_test = import_data_training()\n",
    "\n",
    "IMAGE_PIXELS = 784\n",
    "CLASSES = 10\n",
    "\n",
    "network_params = {'LearningRate':LEARNING_RATE, 'reg':REGULARIZATION, 'HiddenLayerNum':\\\n",
    "                  HIDDEN_LAYER_NUM,'HiddenLayerSize': HIDDEN_LAYER_SIZE, 'log_file_path':TRAIN_DIR}\n",
    "\n",
    "FNN = FNN_Model(CLASSES, IMAGE_PIXELS, network_params)\n",
    "\n",
    "for epoch in range(training_epoch):\n",
    "    avg_cost = 0\n",
    "    NumberOfInputs, _ = x_train.shape\n",
    "    total_batch = int(NumberOfInputs / BATCH_SIZE)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        Input_batch = x_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        Output_batch = y_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        cost = FNN.train_step(Input_batch,Output_batch)\n",
    "        avg_cost += cost/total_batch\n",
    "    print(\"Epoch: {}, cost = {}\".format(epoch, avg_cost))\n",
    "# save_nn_model(FNN, SAVING_PATH)\n",
    "# test model\n",
    "Estimated_output = FNN.predict(x_test, y_test)\n",
    "correct_prediction = np.equal(np.argmax(Estimated_output,1), np.argmax(y_test,1))\n",
    "# print(correct_prediction)\n",
    "accuracy = np.sum(correct_prediction.astype(float))/np.size(correct_prediction)\n",
    "print('Accuracy is {}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "# le.fit(np.unique(y_train))\n",
    "le.fit(y_train)\n",
    "# le.fit(y_train.astype(str))\n",
    "y_2 = le.transform(y_train)\n",
    "\n",
    "# y_2.reshape(len(y_2), 1)\n",
    "\n",
    "oe = preprocessing.OneHotEncoder(categories='auto')\n",
    "# oe = preprocessing.OneHotEncoder()\n",
    "oe.fit(y_2.reshape(len(y_2), 1))\n",
    "oe.transform(y_train.reshape(len(y_train),1)).toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pickle\n",
    "# from collections import deque\n",
    "\n",
    "class FNN:\n",
    "    \"\"\"\n",
    "    neural network model\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions, STATES_SHAPE, params={}):\n",
    "        self.num_actions = num_actions\n",
    "        self.state_shape = STATES_SHAPE\n",
    "        self.LEARNING_RATE = params['LearningRate']\n",
    "        self.reg = params['reg']\n",
    "\n",
    "        self.num_hidden = params['HiddenLayerNum']\n",
    "        self.hidden_size = params['HiddenLayerSize']\n",
    "        self.log_file_path = params['log_file_path']\n",
    "        self.session = self.create_model()\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        input_placeholder = tf.placeholder(tf.float32, shape = (None, self.state_shape))\n",
    "        output_placeholder = tf.placeholder(tf.float32, shape = (None, self.num_actions))\n",
    "        return input_placeholder, output_placeholder\n",
    "\n",
    "\n",
    "    def variable_summaries(self,var, name):\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "\n",
    "    def multilayer_perceptron(self, input_mat):\n",
    "        n_input = self.state_shape\n",
    "        n_hidden_1 = self.hidden_size\n",
    "        n_hidden_2 = self.hidden_size\n",
    "        n_classes = self.num_actions\n",
    "\n",
    "        weights = {\n",
    "            'h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1],stddev = 0.1)),\n",
    "            'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2],stddev = 0.1)),\n",
    "            'out': tf.Variable(tf.truncated_normal([n_hidden_2, n_classes],stddev = 0.1))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([n_hidden_1], stddev = 0.1)),\n",
    "            'b2': tf.Variable(tf.random_normal([n_hidden_2], stddev = 0.1)),\n",
    "            'out': tf.Variable(tf.random_normal([n_classes], stddev = 0.1))\n",
    "        }\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(input_mat, weights['h1']), biases['b1'])\n",
    "        # mean,variance = tf.nn.moments(layer_1,[0])\n",
    "        # layer_1 = tf.nn.batch_normalization(layer_1,mean,variance,None,None,0.00001)\n",
    "        # layer_1 = tf.tanh(layer_1)\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        # mean,variance = tf.nn.moments(layer_2,[0])\n",
    "        # layer_2 = tf.nn.batch_normalization(layer_2,mean,variance,None,None,0.00001)\n",
    "        # layer_2 = tf.tanh(layer_2)\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        reg_term = self.reg * (tf.reduce_sum(tf.square(weights['h1']))+\n",
    "        tf.reduce_sum(tf.square(weights['h2']))+tf.reduce_sum(tf.square(weights['out'])))\n",
    "        return out_layer, reg_term, weights, biases\n",
    "\n",
    "    def create_model(self):\n",
    "        self.input_placeholder, self.output_placeholder = self.define_placeholders()\n",
    "        output, reg_term, weights, biases = self.multilayer_perceptron(self.input_placeholder)\n",
    "\n",
    "        self.variable_summaries(weights['h1'], 'layer1'+ '/weights')\n",
    "        self.variable_summaries(weights['h2'], 'layer2' + '/weights')\n",
    "        self.variable_summaries(weights['out'], 'layer3' + '/weights')\n",
    "\n",
    "        self.NN_output = output\n",
    "\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output,self.output_placeholder))\n",
    "\n",
    "        tf.scalar_summary('cost', self.cost)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = self.LEARNING_RATE)\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        init = tf.initialize_all_variables()\n",
    "\n",
    "        \"\"\"\n",
    "        Default runs at cpu,\n",
    "\n",
    "        For GPU runnning, it takes 33 percent of entire gpu memory\n",
    "        use this:\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "        session = tf.Session(config=tf.ConfigProto(gpu_options = gpu_options))\n",
    "        \"\"\"\n",
    "        session = tf.Session()\n",
    "        session.run(init)\n",
    "        self.saver_op = tf.train.Saver()\n",
    "        self.merged = tf.merge_all_summaries()\n",
    "        self.train_writer = tf.train.SummaryWriter('dqn_log_files/log1/data3',session.graph)\n",
    "        return session\n",
    "\n",
    "    def init_t(self):\n",
    "        self.t = 0\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # print(path)\n",
    "        save_path = self.saver_op.save(self.session, path)\n",
    "        print ('Model saved in file:%s' %save_path)\n",
    "        # pass\n",
    "\n",
    "    def restore_model(self, path):\n",
    "        self.saver_op.restore(self.session, path)\n",
    "\n",
    "\n",
    "    def train_step(self, Input_mat, actions):\n",
    "        _, cost, prediction_probs, self.summary = self.session.run(\n",
    "        [self.train_op, self.cost, self.NN_output, self.merged],\n",
    "        feed_dict = {\n",
    "            self.input_placeholder : Input_mat,\n",
    "            self.output_placeholder : actions\n",
    "        })\n",
    "        return cost\n",
    "        # print(tf.shape(prediction_probs))\n",
    "        # print(tf.shape(actions))\n",
    "        #tensorboard --logdir='dqn_log_files/log1/data1'\n",
    "\n",
    "    def predict(self, states, label):\n",
    "        cost, prediction_probs = self.session.run(\n",
    "        [self.cost, self.NN_output],\n",
    "        feed_dict = {\n",
    "        self.input_placeholder: states,\n",
    "        self.output_placeholder: label\n",
    "        })\n",
    "        return prediction_probs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-MyTensorFlow]",
   "language": "python",
   "name": "conda-env-.conda-MyTensorFlow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
